{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7de910fd",
   "metadata": {},
   "source": [
    "# Programação Dinâmica Estocástica\n",
    "## Avaliação 1\n",
    "\n",
    "1. Considere um programa dinâmico estocástico com horizonte infinito descrito pelos seguintes\n",
    "elementos:\n",
    "\n",
    "- Espaço de estados $\\mathcal{S} = \\{s_1, s_2 \\}$\n",
    "- Conjuntos de decisões $\\mathcal X (s_1) = \\{x_{11}, x_{12} \\}$ e $\\mathcal X (s_{2}) = \\{x_{21}, x_{22} \\}$.\n",
    "- Função probabilidade de transição de estados dada por:\n",
    "\n",
    "$$p(s_{1}|s_{1}, x{11}) = 0.5, \\;\\; p(s_{2}|s_{1}, x{11}) = 0.5$$\n",
    "$$p(s_{1}|s_{1}, x{12}) = 0.7, \\;\\; p(s_{2}|s_{1}, x{12}) = 0.3$$\n",
    "$$p(s_{1}|s_{2}, x{21}) = 0.1, \\;\\; p(s_{2}|s_{2}, x{21}) = 0.9$$\n",
    "$$p(s_{1}|s_{2}, x{22}) = 0.9, \\;\\; p(s_{2}|s_{2}, x{22}) = 0.1$$\n",
    "\n",
    "- Função recompensa dada por:\n",
    "\n",
    "$$r(s1, x11) = r(s1, x12) = 1$$\n",
    "$$r(s2, x21) = r(s2, x22) = 0$$\n",
    "\n",
    "- Fator de desconto $\\gamma = 0.9$\n",
    "\n",
    "Note que neste exercício utiliza-se diretamente a função probabilidade de transição de estados $p(s'|s, x)$, em vez da função de transição de estados $s′ = f(s, x, w)$ e distribuição de probabilidades $P(W = w|s, x)$. Note também que a função recompensa retorna diretamente a recompensa esperada, ou seja:\n",
    "\n",
    "$$r(s, x) = \\mathbb E \\left[r \\left(s, x, W \\right)|s, x \\right], \\forall s \\in \\mathcal S, x \\in \\mathcal X (s),$$\n",
    "\n",
    "em que o valor esperado é computado em relação à distribuição de probabilidades da fonte de aleatoriedade $P(W = w|s, x)$.\n",
    "\n",
    "Responda aos seguintes itens:\n",
    "\n",
    "a. Aproxime a função de valor ótima por meio do método de iteração de valor. Adote $\\epsilon = 0.01$.\n",
    "\n",
    "b. Obtenha a política correspondente à função de valor aproximada no item anterior.\n",
    "\n",
    "c. Obtenha uma política ótima por meio do método de iteração de política. Analise a política e comente se ela parece razoável considerando os dados do problema.\n",
    "\n",
    "d. Compare a política obtida por meio do método da iteração de valor e do método da iteração de política. As políticas coincidem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9dc07f",
   "metadata": {},
   "source": [
    "## Importando Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8cb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201e6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "sns.set_theme(style='darkgrid')\n",
    "sns.set_palette(\"twilight_shifted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac226f",
   "metadata": {},
   "source": [
    "## Definindo parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4137e80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = list(range(2))\n",
    "\n",
    "actions = list(range(2))\n",
    "\n",
    "# State transition probability function\n",
    "# Dimentions: new state x current state x decision\n",
    "p_transition = np.array([[[0.5, 0.7],\n",
    "                            [0.1, 0.9]],\n",
    "                            [[0.5, 0.3],\n",
    "                            [0.9, 0.1]]])\n",
    "\n",
    "# Reward\n",
    "# Dimentions: current state x decision\n",
    "r = np.array([[1., 1.],\n",
    "               [0., 0.]],)\n",
    "\n",
    "# Discount Factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Probability of transictioning to states 1 and 2 if choose decision 1 in state 2\n",
    "p_transition[:, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99010c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = np.zeros([2])\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590938c",
   "metadata": {},
   "source": [
    "### a. Função de valor ótima por meio do método de iteração de valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d591927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fc6908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.64348677, 6.80266129])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [[], []]\n",
    "loss_history = [[], []]\n",
    "while True:\n",
    "    delta = 0\n",
    "\n",
    "    for s in states:\n",
    "        v_old = V[s]\n",
    "        V[s] = max([r[s, a] + gamma * sum(p_transition[s_, s, a]*V[s_] for s_ in states) for a in actions])\n",
    "        \n",
    "        history[s].append(V[s])\n",
    "        delta = max(delta, abs(v_old-V[s]))\n",
    "        loss_history[s].append(delta)\n",
    "    \n",
    "    if delta < epsilon:\n",
    "        break\n",
    "\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f82c56",
   "metadata": {},
   "source": [
    "### b. Política correspondente à função de valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f69d4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Policy\n",
    "pi = np.zeros([2])\n",
    "\n",
    "for s in states:\n",
    "    pi[s] = np.argmax([r[s, a] + gamma * sum(p_transition[s_, s, a]*V[s_] for s_ in states) for a in actions])\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61015fb",
   "metadata": {},
   "source": [
    "### c. Política ótima por meio do método de iteração de política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc34be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
